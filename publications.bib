
@inproceedings{luo_texture_2023,
	address = {Sydney NSW Australia},
	title = {Texture {Atlas} {Compression} {Based} on {Repeated} {Content} {Removal}},
	isbn = {9798400703157},
	url = {https://dl.acm.org/doi/10.1145/3610548.3618150},
	doi = {10.1145/3610548.3618150},
	language = {en},
	urldate = {2023-12-20},
	booktitle = {{SIGGRAPH} {Asia} 2023 {Conference} {Papers}},
	publisher = {ACM},
	author = {Luo, Yuzhe and Jin, Xiaogang and Pan, Zherong and Wu, Kui and Kou, Qilong and Yang, Xiajun and Gao, Xifeng},
	month = dec,
	year = {2023},
	pages = {1--11},
	file = {Luo et al_2023_Texture Atlas Compression Based on Repeated Content Removal.pdf:files/4856/Luo et al_2023_Texture Atlas Compression Based on Repeated Content Removal.pdf:application/pdf},
}


@article{yang_mnss_2023,
	title = {{MNSS}: {Neural} {Supersampling} {Framework} for {Real}-{Time} {Rendering} on {Mobile} {Devices}},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{MNSS}},
	url = {https://ieeexplore.ieee.org/document/10076842/},
	doi = {10.1109/TVCG.2023.3259141},
	abstract = {Although neural supersampling has achieved great success in various applications for improving image quality, it is still difficult to apply it to a wide range of real-time rendering applications due to the high computational power demand. Most existing methods are computationally expensive and require high-performance hardware, preventing their use on platforms with limited hardware, such as smartphones. To this end, we propose a new supersampling framework for real-time rendering applications to reconstruct a high-quality image out of a low-resolution one, which is sufficiently lightweight to run on smartphones within a real-time budget. Our model takes as input the renderer-generated low resolution content and produces high resolution and anti-aliased results. To maximize sampling efficiency, we propose using an alternate sub-pixel sample pattern during the rasterization process. This allows us to create a relatively small reconstruction model while maintaining high image quality. By accumulating new samples into a high-resolution history buffer, an efficient history check and re-usage scheme is introduced to improve temporal stability. To our knowledge, this is the first research in pushing real-time neural supersampling on mobile devices. Due to the absence of training data, we present a new dataset containing 57 training and test sequences from three game scenes. Furthermore, based on the rendered motion vectors and a visual perception study, we introduce a new metric called inter-frame structural similarity (IF-SSIM) to quantitatively measure the temporal stability of rendered videos. Extensive evaluations demonstrate that our supersampling model outperforms existing or alternative solutions in both performance and temporal stability.},
	language = {en},
	urldate = {2023-11-27},
	journal = {IEEE Trans. Visual. Comput. Graphics},
	author = {Yang, Sipeng and Zhao, Yunlu and Luo, Yuzhe and Wang, He and Sun, Hongyu and Li, Chen and Cai, Binghuang and Jin, Xiaogang},
	year = {2023},
	pages = {1--14},
	file = {Yang 等 - 2023 - MNSS Neural Supersampling Framework for Real-Time.pdf:C\:\\Users\\royyuzheluo\\Zotero\\storage\\QW97TSV5\\Yang 等 - 2023 - MNSS Neural Supersampling Framework for Real-Time.pdf:application/pdf},
}

@article{ying_glyphcreator_2022,
	title = {{GlyphCreator}: {Towards} {Example}-based {Automatic} {Generation} of {Circular} {Glyphs}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{GlyphCreator}},
	url = {https://ieeexplore.ieee.org/document/9557223/},
	doi = {10.1109/TVCG.2021.3114877},
	abstract = {Circular glyphs are used across disparate ﬁelds to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we ﬁrst derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {IEEE Trans. Visual. Comput. Graphics},
	author = {Ying, Lu and Tang, Tan and Luo, Yuzhe and Shen, Lvkeshen and Xie, Xiao and Yu, Lingyun and Wu, Yingcai},
	month = jan,
	year = {2022},
	pages = {400--410},
	file = {Ying 等 - 2022 - GlyphCreator Towards Example-based Automatic Gene.pdf:C\:\\Users\\royyuzheluo\\Zotero\\storage\\AAWEL8RF\\Ying 等 - 2022 - GlyphCreator Towards Example-based Automatic Gene.pdf:application/pdf},
}